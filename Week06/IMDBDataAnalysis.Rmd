---
title: "Homework 3 - IMDB Data Analysis & Project 1 Reflection"
author: "Arun Mahadevan Sathia Narayanan"
date: 'Date Published: 2024-09-28'
output: rmdformats::readthedown
---
<style type="text/css">
.main-container {
  max-width: 100% !important;
  margin: auto;
}
</style>
# PART 1  
```{r echo = FALSE, include = FALSE}
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
tv_series = read.csv("Homework3_tvSeries.csv")
principals = read.csv("Homework3_principals.csv")
ratings = read.csv("Homework3_ratings.csv")
```
  
### Question 1  
Create a dataframe that contains all of the TV series that are no longer running (meaning they have an end year in the dataset).  
Create a new variable to keep track of how many years each series ran and display the distribution of this variable.  
How many TV series in this dataset ended the same year as they started?
```{r echo = FALSE, include = FALSE}
# Question 1
# How many TV series have ended?
tv_series_ended <- data.frame(tv_series)
tv_series_ended <- tv_series_ended |> 
  filter(endYear != "NA") |>
  mutate(runTime = endYear - startYear + 1)
count = length(tv_series_ended$endYear)
```
  
How many TV series have ended?
```{r echo = FALSE}
count
```
  
How long did each show run for?
```{r echo = FALSE}
year_length_plot <- ggplot(tv_series_ended) + 
  geom_bar(aes(y = runTime), color = "gold", fill = "gold") + 
  labs(
    title = "Runtime of Shows",
    x = "Count",
    y = "Runtime (years)"
  ) + 
  theme(panel.background = element_rect(fill = 'black', colour = 'gold'), panel.grid.minor = element_blank()) +
  scale_y_continuous(breaks = seq(0, 9, by = 1))
year_length_plot
```
  
How many TV series have ended the same year they began?
```{r echo = FALSE}
# How many TV series have ended the same year they began?
count_same_year = length((tv_series_ended |> 
                            filter(runTime == 1))
                         $startYear)
count_same_year
```
  
### Question 2  
What proportion of the TV series have rating information?  
Find the TV series with a rating of 9 or higher with at least 20,000 votes and print the series names, sorted alphabetically.
```{r echo = FALSE, include = FALSE}
# Question 2
# What proportion of the TV series have rating information?
ratings_series <- merge(ratings, tv_series, by = "tconst")
prop_rating = round(length(ratings_series$tconst) / length(tv_series$tconst) * 100, 2)
```
  
What proportion of the TV series have rating information?
```{r echo = FALSE}
prop_rating
```

```{r echo = FALSE, include = FALSE}
# TV series with a rating of 9+ and votes of 20,000+, sorted alphabetically (only names)
ratings_mins <- data.frame(ratings)
ratings_mins <- ratings_mins |>
  filter(averageRating >= 9) |>
  filter(numVotes >= 20000)
ratings_mins <- merge(ratings_mins, tv_series, by = "tconst")
ratings_mins <- ratings_mins |>
  arrange(primaryTitle) |>
  select(averageRating, numVotes, primaryTitle) |>
  summarize("Average Rating" = averageRating, "Number of Votes" = numVotes, "Primary Title" = primaryTitle)
```
  
TV series with a rating of 9 or higher and with at least 20,000 votes, sorted alphabetically:  
```{r echo = FALSE}
ratings_mins
```
  
### Question 3  
Calculate the ages of the principal cast and crew when each TV series started.  
Compare the median ages for each category of principals at the start of these TV series.  
Note: the "self" category denotes people playing themselves, e.g. on reality TV shows.
```{r echo = FALSE, include = FALSE}
# Question 3
# Ages of the principal cast and crew when each TV series started
filtered_principal <- data.frame(principals)
filtered_principal <- merge(filtered_principal, tv_series, by = "tconst") |>
  select(tconst, nconst, category, primaryName, birthYear, startYear) |>
  filter(birthYear != "NA") |>
  mutate(startAge = startYear - birthYear) |>
  arrange(category, startAge) |>
  select(primaryName, category, birthYear, startYear, startAge)
filtered_principal_formatted <- filtered_principal |>
  summarize("Name" = primaryName, "Category" = category, "Birth Year" = birthYear, "Start Year" = startYear, "Age" = startAge)
```
  
A few of the ages of the principal cast and crew when each TV series started:
```{r echo = FALSE}
head(filtered_principal_formatted)
```

```{r echo = FALSE, include = FALSE}
# Formatting of the filtered_principal and getting the median ages per category (actor, actress, self, writer)
filtered_principal_age <- filtered_principal |>
  group_by(category) |>
  select(category, startAge) |>
  summarize("MedianAge" = median(startAge, na.rm = T)) |>
  rename("Category" = category)
filtered_principal_age <- data.frame(filtered_principal_age)
colnames(filtered_principal_age)[2] = "Median Age"
```
  
Getting the median ages per category (actor, actress, self, writer):
```{r echo = FALSE}
filtered_principal_age
```

# PART 2  
### Question 4  
**Why did you choose the dataset you used for this project? What about the context of the data interested you?  Why did you choose the predictors you did, and what did you expect to find in their relationships with the outcome variable?**  
I chose the Tornado dataset for this project because tornadoes are generally an interesting topic. The context of the data, being property loss caused by each tornado and if there were any injuries or fatalities per tornado, is a really viable topic for the modern day, especially given the fact that we live in Texas (a state that is part of the infamous Tornado Alley), as it could be used to design new materials that houses and buildings could be constructed with to minimize damage. The predictors I chose -- state, magnitude, and the length traveled -- each have their own relativity to tornadoes:  
1) the location determines population and community cluster, which affects the number of houses destroyed and leads to a higher property loss,  
2) the magnitude determines the strength of the tornado, which means more property could have been damaged in the same time a tornado with a lower magnitude would have, and  
3) the length (distance) a tornado travels has an effect on how much property the tornado goes through, with a longer distance causing more property loss.  
For each of the three predictors, I expected the following:  
1) the locations with more tornadoes, in my case I analyzed the Tornado Alley and proved its existence, should have more property damage (as they can have more tornadoes with a higher magnitude).  
2) the magnitude is simply strength: the stronger the tornado, the more property loss incurred.  
3) the length is an obvious one to think of: the longer the tornado travels, the more damage that is caused.  

### Question 5
**What challenges did you face while completing the assignment, and how did you overcome those challenges?  What do you plan to do differently next time as a result?  List at least one specific thing you learned about yourself as a data science student as a result.**  
The main challenge I faced while completing the assignment was making the report engaging. All too well I know the feeling of reading though an article but just being bored while reading it and not retaining any information. To prevent this from happening, I had to find the right balance between information and visuals. I had to quickly switch between the two, analyzing the graph then nearly immediately presenting a new one. The problem with this was that there would be too many visuals being presented, and not enough time can be taken to truly understand what is happening in the graphs. For this, I had to often take breaks from the information, and try to communicate with the reader, whether it be a quick lesson on some topics, or just a cringe joke every now and then. The focus remained on the data however, and I made sure to help the reader analyze the data as much as possible. While reading though the article, I realized that some parts do end up boring anyway. I tried this out with others, and the net result was the same: there would just be constant scrolling and less attention to the details presented in the text. Next time, it is crucial that I include fewer words in the article, and only have sharp, crisp, and clean data presented in the article. <br>   
There is growth from this, which is understanding that pure data bores anybody, including myself. When effort is put into communicating effectively what is happening in each graph or explaining a topic not well known to the average reader, it helps the reader keep moving along your article rather than get distracted by specific terms or just close the article altogether. Overall, I realized that Iâ€™m good at balancing technical details with engaging communication, but there is improvement along the way that is necessary.  

### Question 6  
**Describe at least one way your personal background influenced how you analyzed this data or presented your analysis. What might someone else have done differently if analyzing the same dataset?**  
As explained before, I get bogged down in the details with any article of just pure data. I wanted to mitigate this for other readers, and thus I presented it the way I did. Somebody else with a different background -- say a person is completely involved in data and not much on the reading aspect of the article -- would have presented the data with just the bucketload of data and not putting much explanation behind the graphs and whatever else could be derived from the graphs. On the flip side, say a person hates the details (which I would say is very uncommon among the DATA SCIENCE group, but let's roll with it for the situation), would include more generals to appeal to a larger audience, and not discuss the data as much.